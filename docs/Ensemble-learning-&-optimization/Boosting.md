
# Boosting (Ensemble Learning Technique)

## ðŸ”¹ Definition:
Boosting is a **sequential ensemble method** that combines multiple **weak learners** (models with high bias & low variance) to form a **strong predictive model**.

---

## ðŸ”¹ Key Differences: Bagging vs. Boosting

| Feature          | Bagging                          | Boosting                     |
|------------------|----------------------------------|------------------------------|
| Model Addition   | Parallel, independent             | Sequential, dependent        |
| Focus            | Reduce variance                  | Reduce bias & variance       |
| Data Sampling    | Bootstrap samples                 | Reweighted samples (focus on errors) |
| Voting Type      | Equal (majority voting)           | Weighted (Î±â‚œ-weighted voting) |
| Learner Type     | Unstable learners (e.g., trees)   | Weak learners                |

---

## ðŸ”¹ How Boosting Works (Step-by-Step):

1. Train a **weak learner** on the dataset.
2. Identify and focus on **errors/misclassifications**.
3. Update/reweight training data to emphasize errors.
4. Train next model to correct prior errors.
5. Continue adding models iteratively.
6. Final model is a **weighted combination** of all weak learners:

```
F(x) = Î£ Î±â‚œ hâ‚œ(x)
```

- **hâ‚œ(x):** Weak learner at iteration *t*  
- **Î±â‚œ:** Contribution (higher for better learners)

---

## ðŸ”¹ Why Weighted Averaging (Î±â‚œ)?

- Not all models are equally good.
- **Î±â‚œ** gives higher weight to stronger models, similar to valuing expert advice over guesses.
- Ensures accurate models have stronger influence in the final prediction.

---

## ðŸ”¹ Mathematical Foundation:

Boosting solves an **optimization problem**:

Minimizes overall **loss function** via **stagewise additive modeling**:
- Each iteration adds a model that reduces residual error without changing previous models.

Formula:
```
F(x) = min Î£ L(yáµ¢, Î£ Î±â‚œ hâ‚œ(x))
```

Where:
- **L:** Loss function (e.g., squared error, log loss)
- Solution via **forward stagewise additive modeling** (practically feasible).

---

## ðŸ”¹ Advantages:
- Reduces both **bias and variance**.
- High predictive performance, especially in complex tasks.
- Works well for imbalanced or noisy datasets.
