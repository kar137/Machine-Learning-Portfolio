
# Boosting (Ensemble Learning Technique)

## üîπ Definition:
Boosting is a **sequential ensemble method** that combines multiple **weak learners** (models with high bias & low variance) to form a **strong predictive model**.

---

## üîπ Key Differences: Bagging vs. Boosting

| Feature          | Bagging                          | Boosting                     |
|------------------|----------------------------------|------------------------------|
| Model Addition   | Parallel, independent             | Sequential, dependent        |
| Focus            | Reduce variance                  | Reduce bias & variance       |
| Data Sampling    | Bootstrap samples                 | Reweighted samples (focus on errors) |
| Voting Type      | Equal (majority voting)           | Weighted (Œ±‚Çú-weighted voting) |
| Learner Type     | Unstable learners (e.g., trees)   | Weak learners                |

---

## üîπ How Boosting Works (Step-by-Step):

1. Train a **weak learner** on the dataset.
2. Identify and focus on **errors/misclassifications**.
3. Update/reweight training data to emphasize errors.
4. Train next model to correct prior errors.
5. Continue adding models iteratively.
6. Final model is a **weighted combination** of all weak learners:

```
F(x) = Œ£ Œ±‚Çú h‚Çú(x)
```

- **h‚Çú(x):** Weak learner at iteration *t*  
- **Œ±‚Çú:** Contribution (higher for better learners)

---

## üîπ Why Weighted Averaging (Œ±‚Çú)?

- Not all models are equally good.
- **Œ±‚Çú** gives higher weight to stronger models, similar to valuing expert advice over guesses.
- Ensures accurate models have stronger influence in the final prediction.

---

## üîπ Mathematical Foundation:

Boosting solves an **optimization problem**:

Minimizes overall **loss function** via **stagewise additive modeling**:
- Each iteration adds a model that reduces residual error without changing previous models.

Formula:
```
F(x) = min Œ£ L(y·µ¢, Œ£ Œ±‚Çú h‚Çú(x))
```

Where:
- **L:** Loss function (e.g., squared error, log loss)
- Solution via **forward stagewise additive modeling** (practically feasible).

---

## üîπ Advantages:
- Reduces both **bias and variance**.
- High predictive performance, especially in complex tasks.
- Works well for imbalanced or noisy datasets.

# AdaBoost (Adaptive Boosting)

## üîπ Definition:
AdaBoost (Adaptive Boosting) is one of the first and most popular boosting algorithms, initially designed for **binary classification** and later extended to regression.

It builds an ensemble by **sequentially training weak learners** (usually decision stumps) and adjusting weights on training instances to focus more on misclassified samples.

---

## üîπ How AdaBoost Works:

1. Assign equal weights to all training instances.
2. Train a weak learner on the dataset.
3. Increase weights of misclassified instances; decrease weights of correctly classified ones.
4. Train next model using updated weights, focusing more on hard instances.
5. Continue iteratively adding models.
6. Final prediction combines all models weighted by their accuracy (contribution factor Œ±‚Çú).

---

## üîπ Mathematical Formulation:

### Loss Function:
Uses **exponential loss** for binary classification:
L(y, F(x)) = exp(-y * F(x))

shell
Copy
Edit

### Optimization Objective:
Find weak learner h‚Çú and Œ±‚Çú that minimize the weighted exponential loss:
Œ±‚Çú, h‚Çú = argmin Œ£ exp(-y·µ¢ * (F‚Çú‚Çã‚ÇÅ(x·µ¢) + Œ± * h(x·µ¢)))

### Weighted Error:
err‚Çú = Œ£ w·µ¢ * I(y·µ¢ ‚â† h‚Çú(x·µ¢))

### Compute Œ±‚Çú:
Œ±‚Çú = 0.5 * log((1 - err‚Çú) / err‚Çú)

### Model Update:
F‚Çú(x) = F‚Çú‚Çã‚ÇÅ(x) + Œ±‚Çú * h‚Çú(x)

### Weight Update:
w·µ¢(new) = w·µ¢ * exp(-Œ±‚Çú * y·µ¢ * h‚Çú(x·µ¢))

---

## üîπ Intuition:

- Focuses each new model on the instances misclassified by the previous model.
- Misclassified instances get higher weights; correctly classified instances get lower weights.
- Final prediction is a **weighted majority vote**.

---

## üîπ Why Exponential Loss?
- Penalizes misclassified instances heavily.
- Naturally leads to weight updates in AdaBoost.
- Makes optimization tractable with weighted error minimization.

---

## üîπ AdaBoost for Regression:
- Uses different loss functions, e.g., squared loss.
- Weak learners output continuous values instead of binary predictions.


# Gradient Boosting

## 1. Concept Overview
Gradient Boosting is an ensemble technique that combines **boosting** with **gradient descent** to minimize errors in predictive models.

- **Boosting**: Sequentially adds weak learners to correct errors from previous models.
- **Gradient Descent**: Uses gradients of the loss function to minimize error.

---

## 2. Key Intuition
- AdaBoost identifies model errors by assigning higher weights to misclassified points.
- Gradient Boosting identifies errors by computing the **gradient of the loss function**.
- The model is updated iteratively by adding new models trained on the negative gradient (residuals in case of squared loss).

---

## 3. Workflow (Regression Example)
1. Start with an initial model (usually the mean).
2. Compute residuals (errors).
3. Train a new model on residuals.
4. Update the model:  
   `F_new(x) = F_old(x) + Œ± * h(x)`  
   *(Œ± = learning rate/shrinkage)*
5. Repeat for several iterations.

---

## 4. Gradient Connection
- In squared loss, residuals = negative gradients.
- Gradient Boosting generalizes this concept to **any differentiable loss function**.

---

## 5. Loss Functions (Regression)
- **Squared Loss**: Sensitive to outliers.
- **Absolute Loss**: Robust to outliers but less smooth for optimization.
- **Huber Loss**: Combines squared and absolute loss; robust and smooth.

---

## 6. Gradient Boosting for Classification

### Binary Classification:
- Uses **logistic loss** and works with logits instead of probabilities.
- Boosting is performed on logits, not probabilities:
  - Compute negative gradient of logistic loss.
  - Train model on this gradient.
  - Convert final logits to probabilities using sigmoid function.

#### Key Formula (Binary Classification Gradient):
‚àÇJ/‚àÇF(x) = e^F(x) / (1 + e^F(x)) - y


---

### Multi-Class Classification:
- Uses **cross-entropy loss** and **softmax function**:
  - Each class has its own score function.
  - Gradients are computed w.r.t. each class‚Äôs score.
  - Softmax maps scores to class probabilities.
- Models for all classes are updated at each iteration.

---

## 7. Advantages
- Highly flexible; works with many types of loss functions.
- Handles both regression and classification tasks.
- Huber loss provides robustness to outliers.
- Learning rate helps control overfitting.

---

## 8. Common Use Cases
- Regression tasks with complex, nonlinear relationships.
- Classification tasks requiring high accuracy.
- Scenarios with noisy or outlier-prone data.


# ‚úÖ XGBoost Summary

## üìö Prerequisites
- Decision Trees and their construction  
- Working of Gradient Boosting  

---

## üéØ Learning Objectives
- Explain XGBoost as a fast and scalable machine learning library.
- Understand problems with typical decision trees.
- Understand issues with typical boosting algorithms.
- Explain how XGBoost solves these issues.
- List key features of XGBoost.

---

## XGBoost
**XGBoost (Extreme Gradient Boosting)** is a gradient boosting library focused on speed and performance.

- Published by Chen et al. (2016) in the paper: *XGBoost: A Scalable Tree Boosting System*.
- Known for winning many ML competitions (e.g., Kaggle, KDD Cup).
- Supports regression, classification, and ranking tasks.

---

## Problems with Decision Trees (Solved by XGBoost)
### 1. Greedy Split Finding
- Decision Trees use greedy methods, which are computationally expensive on large datasets.
- XGBoost uses **approximate algorithms**:
  - Uses quantiles as candidate splits (e.g., 10%, 20%, ‚Ä¶, 90%).
  - Reduces computational cost with minimal performance loss.
- Uses **quantile sketch** (weighted variant) for huge datasets.

### 2. Overfitting
- Deep trees overfit easily.
- XGBoost introduces:
  - Regularization (L1 & L2)
  - Row and column subsampling.

### 3. Tree Structure Limitation
- Traditional trees return class labels (not ideal for boosting).
- XGBoost uses **regression trees**:
  - Each leaf predicts a real-valued score (weight).
  - Allows unified handling of classification & regression.

---

## Problems with Boosting (Solved by XGBoost)
### 1. Sequential Nature
- Boosting is inherently sequential (slow).
- XGBoost introduces:
  - Parallelization (within-tree level, not across trees).
  - Distributed computing for large datasets.

---

## ‚ú® Key Features of XGBoost
### 1. Regularization
Adds regularization to reduce overfitting:
Obj = Œ£ L(y·µ¢, F‚Çú(x·µ¢)) + Œ©(h‚Çú)

Where Œ©(h) = Œ≥T + ¬ΩŒª Œ£ w‚±º¬≤  
(T = number of leaves, w‚±º = weight at leaf j).

---

### 2. Second-Order Gradient (Taylor Expansion)
Uses both:
- **First-order gradient (g·µ¢)**  
- **Second-order gradient (h·µ¢, Hessian)**  
‚Üí Faster convergence than traditional methods.

---

### 3. Sparsity Aware Split Finding
- Handles missing values natively.
- Learns default direction during training for missing data.

---

### 4. Stochastic Gradient Boosting
- Uses **row and column subsampling**:
  - `colsample_bytree`
  - `colsample_bylevel`
  - `colsample_bynode`

---

### 5. Shrinkage (Learning Rate)
- Dampens contribution of new trees:
Obj = Œ£ L(y·µ¢, F‚Çú‚Çã‚ÇÅ(x·µ¢) + Œµ h‚Çú(x·µ¢)) + Œ©(h‚Çú)

(Œµ = learning_rate or shrinkage)

---

### 6. Custom Loss Functions
- Allows custom loss functions.
- Examples:
  - Squared Loss (Regression)
  - Logistic Loss (Binary Classification)

---

### 7. System Features
- **Parallelization**: Uses sorted block structure for efficient splits.
- **Distributed Computing**: Multi-machine training support.
- **Cache Optimization**: Gradient & Hessian prefetching.
- **Out-of-Core Computation**: Disk-based training for large datasets.