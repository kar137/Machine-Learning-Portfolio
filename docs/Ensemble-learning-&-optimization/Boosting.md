
# Boosting (Ensemble Learning Technique)

## üîπ Definition:
Boosting is a **sequential ensemble method** that combines multiple **weak learners** (models with high bias & low variance) to form a **strong predictive model**.

---

## üîπ Key Differences: Bagging vs. Boosting

| Feature          | Bagging                          | Boosting                     |
|------------------|----------------------------------|------------------------------|
| Model Addition   | Parallel, independent             | Sequential, dependent        |
| Focus            | Reduce variance                  | Reduce bias & variance       |
| Data Sampling    | Bootstrap samples                 | Reweighted samples (focus on errors) |
| Voting Type      | Equal (majority voting)           | Weighted (Œ±‚Çú-weighted voting) |
| Learner Type     | Unstable learners (e.g., trees)   | Weak learners                |

---

## üîπ How Boosting Works (Step-by-Step):

1. Train a **weak learner** on the dataset.
2. Identify and focus on **errors/misclassifications**.
3. Update/reweight training data to emphasize errors.
4. Train next model to correct prior errors.
5. Continue adding models iteratively.
6. Final model is a **weighted combination** of all weak learners:

```
F(x) = Œ£ Œ±‚Çú h‚Çú(x)
```

- **h‚Çú(x):** Weak learner at iteration *t*  
- **Œ±‚Çú:** Contribution (higher for better learners)

---

## üîπ Why Weighted Averaging (Œ±‚Çú)?

- Not all models are equally good.
- **Œ±‚Çú** gives higher weight to stronger models, similar to valuing expert advice over guesses.
- Ensures accurate models have stronger influence in the final prediction.

---

## üîπ Mathematical Foundation:

Boosting solves an **optimization problem**:

Minimizes overall **loss function** via **stagewise additive modeling**:
- Each iteration adds a model that reduces residual error without changing previous models.

Formula:
```
F(x) = min Œ£ L(y·µ¢, Œ£ Œ±‚Çú h‚Çú(x))
```

Where:
- **L:** Loss function (e.g., squared error, log loss)
- Solution via **forward stagewise additive modeling** (practically feasible).

---

## üîπ Advantages:
- Reduces both **bias and variance**.
- High predictive performance, especially in complex tasks.
- Works well for imbalanced or noisy datasets.

# AdaBoost (Adaptive Boosting)

## üîπ Definition:
AdaBoost (Adaptive Boosting) is one of the first and most popular boosting algorithms, initially designed for **binary classification** and later extended to regression.

It builds an ensemble by **sequentially training weak learners** (usually decision stumps) and adjusting weights on training instances to focus more on misclassified samples.

---

## üîπ How AdaBoost Works:

1. Assign equal weights to all training instances.
2. Train a weak learner on the dataset.
3. Increase weights of misclassified instances; decrease weights of correctly classified ones.
4. Train next model using updated weights, focusing more on hard instances.
5. Continue iteratively adding models.
6. Final prediction combines all models weighted by their accuracy (contribution factor Œ±‚Çú).

---

## üîπ Mathematical Formulation:

### Loss Function:
Uses **exponential loss** for binary classification:
L(y, F(x)) = exp(-y * F(x))

shell
Copy
Edit

### Optimization Objective:
Find weak learner h‚Çú and Œ±‚Çú that minimize the weighted exponential loss:
Œ±‚Çú, h‚Çú = argmin Œ£ exp(-y·µ¢ * (F‚Çú‚Çã‚ÇÅ(x·µ¢) + Œ± * h(x·µ¢)))

### Weighted Error:
err‚Çú = Œ£ w·µ¢ * I(y·µ¢ ‚â† h‚Çú(x·µ¢))

### Compute Œ±‚Çú:
Œ±‚Çú = 0.5 * log((1 - err‚Çú) / err‚Çú)

### Model Update:
F‚Çú(x) = F‚Çú‚Çã‚ÇÅ(x) + Œ±‚Çú * h‚Çú(x)

### Weight Update:
w·µ¢(new) = w·µ¢ * exp(-Œ±‚Çú * y·µ¢ * h‚Çú(x·µ¢))

---

## üîπ Intuition:

- Focuses each new model on the instances misclassified by the previous model.
- Misclassified instances get higher weights; correctly classified instances get lower weights.
- Final prediction is a **weighted majority vote**.

---

## üîπ Why Exponential Loss?
- Penalizes misclassified instances heavily.
- Naturally leads to weight updates in AdaBoost.
- Makes optimization tractable with weighted error minimization.

---

## üîπ AdaBoost for Regression:
- Uses different loss functions, e.g., squared loss.
- Weak learners output continuous values instead of binary predictions.
